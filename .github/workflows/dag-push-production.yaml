name: Build Packages Production using wolfictl

on:
  workflow_dispatch:
  push:
    branches:
      - main

env:
  PROJECT: prod-images-c6e5
  CLUSTER_NAME: tmp-cluster-prod-${{github.run_id}}
  CLUSTER_ZONE: us-central1-b
  SERVICE_ACCOUNT: prod-images-ci
  FQ_SERVICE_ACCOUNT: prod-images-ci@prod-images-c6e5.iam.gserviceaccount.com
  BUCKET: wolfi-production-registry-destination/os/
  SRC_BUCKET: gs://wolfi-production-registry-destination/os/

# Only run one build at a time, to prevent bad signature errors.
concurrency:
  group: aarch64-${{ github.ref }}

jobs:
  setup-cluster:
    name: Setup build cluster
    runs-on: ubuntu-latest
    if: github.repository == 'wolfi-dev/os'

    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v3
        with:
          repository: wolfi-dev/wolfictl
          path: ${{github.workspace}}/wolfictl

      - uses: google-github-actions/auth@v0
        with:
          workload_identity_provider: "projects/618116202522/locations/global/workloadIdentityPools/prod-shared-e350/providers/prod-shared-gha"
          service_account: ${{env.FQ_SERVICE_ACCOUNT}}
      - uses: google-github-actions/setup-gcloud@v0
        with:
          project_id: ${{env.PROJECT}}
      - name: 'Use gcloud CLI'
        run: 'gcloud auth list --filter=status:ACTIVE --format="value(account)"'

      - name: Setup Build Cluster
        working-directory: ${{github.workspace}}/wolfictl
        run: |
          gcloud container clusters create "${CLUSTER_NAME}" \
            --enable-ip-alias \
            --network                       projects/prod-shared-37eb/global/networks/prod-shared-586cca3 \
            --subnetwork                    projects/prod-shared-37eb/regions/us-central1/subnetworks/prod-shared-imgs-us-c1-b9c111f \
            --cluster-secondary-range-name  gke-a-pods \
            --services-secondary-range-name gke-a-svcs \
            --tags                          "egress-inet" \
            --enable-dataplane-v2 \
            --enable-intra-node-visibility \
            --service-account "${FQ_SERVICE_ACCOUNT}" \
            --zone            "${CLUSTER_ZONE}" \
            --release-channel rapid \
            --workload-pool   "${PROJECT}.svc.id.goog" \
            --machine-type    e2-standard-4 \
            --num-nodes       1 \
            --spot  \
            --no-enable-autorepair

          gcloud container node-pools create arm-nodes \
            --cluster         "${CLUSTER_NAME}" \
            --zone            "${CLUSTER_ZONE}" \
            --tags            "egress-inet" \
            --service-account "${FQ_SERVICE_ACCOUNT}" \
            --machine-type    t2a-standard-32 \
            --disk-size=700GB \
            --num-nodes       1 \
            --spot  \
            --no-enable-autorepair

      - uses: imjasonh/gke-auth@v0.2.0
        with:
          project: "${PROJECT}"
          location: "${CLUSTER_ZONE}"
          cluster: "${CLUSTER_NAME}"

      - working-directory: ${{github.workspace}}/wolfictl
        run: ./scripts/setup-cluster.sh ${SERVICE_ACCOUNT}

      # Dogfood our own CSI secrets-store images.
      # These can introduce a bootstrapping issue: if the images are broken,
      # we won't be able to use them to build a fixed replacement.
      # In that case, comment this out to use the upstream directly.
      - run: |
          kubectl set image daemonset/csi-secrets-store \
            -n kube-system \
            secrets-store=cgr.dev/chainguard/secrets-store-csi-driver:latest@sha256:c45a6ba62dc23a31a837580af7a2d985499d081bd62e1fa5e713634ebca3ef1a

          kubectl set image daemonset/csi-secrets-store-provider-gcp \
            -n kube-system \
            provider=cgr.dev/chainguard/secrets-store-csi-driver-provider-gcp:latest@sha256:b7033fc1e0f371f9d809ad0adbbef3c5247fc82105ef9546ba2d7e67b586e909

          # Wait for DaemonSets to become ready.
          kubectl rollout status daemonset -n kube-system csi-secrets-store
          kubectl rollout status daemonset -n kube-system csi-secrets-store-provider-gcp

  # TODO: Update source cache here, instead of in a separate workflow.
  #       This likely depends on making the source cache bucket configurable by
  #       wolfictl, or if we have staging builds reuse the prod source cache, then
  #       we only need to update it before prod builds.

  # TODO: Add build-amd64

  build-arm64:
    name: Build (arm64)
    runs-on: ubuntu-latest
    needs: setup-cluster

    permissions:
      id-token: write
      contents: read

    steps:
      # Checkout and build wolfictl from main
      # Can't `go install` because its go.mod has `replace`s.
      - uses: actions/setup-go@v3
        with:
          go-version: '>=1.19.0'
      - uses: actions/checkout@v3
        with:
          repository: wolfi-dev/wolfictl
          path: ${{github.workspace}}/wolfictl
      - working-directory: ${{github.workspace}}/wolfictl
        run: go install

      - uses: google-github-actions/auth@v0
        with:
          workload_identity_provider: "projects/618116202522/locations/global/workloadIdentityPools/prod-shared-e350/providers/prod-shared-gha"
          service_account: ${{env.FQ_SERVICE_ACCOUNT}}
      - uses: google-github-actions/setup-gcloud@v0
        with:
          project_id: ${{env.PROJECT}}
      - uses: 'google-github-actions/get-gke-credentials@v0'
        with:
          cluster_name: ${{ env.CLUSTER_NAME }}
          location: ${{ env.CLUSTER_ZONE }}
      - run: gcloud auth configure-docker --quiet

      - uses: actions/checkout@v3
        with:
          repository: wolfi-dev/os
          path: ${{github.workspace}}/os

      - working-directory: ${{github.workspace}}/os
        run: |
          wolfictl pod \
            --cpu=30 --ram=100Gi \
            --bucket=${BUCKET} \
            --src-bucket=${SRC_BUCKET} \
            --sdk-image ghcr.io/wolfi-dev/sdk:latest@sha256:b1fa7ba0b6bfd5350e666ffd0af4d3ef52b1f16b18f253952c181ca44a4dd901 \
            --pending-timeout=20m \
            --secret-key \
            --arch=arm64

  teardown-cluster:
    name: Teardown build cluster
    runs-on: ubuntu-latest
    needs: [build-arm64]  # TODO: Add build-amd64
    if: always() && github.repository == 'wolfi-dev/os'

    permissions:
      id-token: write
      contents: read

    steps:
      - uses: google-github-actions/auth@v0
        with:
          workload_identity_provider: "projects/618116202522/locations/global/workloadIdentityPools/prod-shared-e350/providers/prod-shared-gha"
          service_account: ${{env.FQ_SERVICE_ACCOUNT}}
      - uses: google-github-actions/setup-gcloud@v0
        with:
          project_id: ${{env.PROJECT}}
      - uses: 'google-github-actions/get-gke-credentials@v0'
        with:
          cluster_name: ${{ env.CLUSTER_NAME }}
          location: ${{ env.CLUSTER_ZONE }}

      - name: Collect diagnostics
        if: always()
        run: |
          resources="pods daemonsets serviceaccounts namespaces"
          for ns in $(kubectl get ns -oname | cut -d'/' -f 2); do
            for resource in ${resources}; do
              echo --- $ns $resource ---
              kubectl get $resource -n${ns}
              for x in $(kubectl get $resource -n${ns} -oname || true); do
                echo "::group:: describe $resource $x"
                # Don't fail if the resource disappears midway.
                kubectl describe -n${ns} $x || true
                echo '::endgroup::'
              done
            done
          done

      - name: Teardown Build Cluster
        if: always()
        run: |
          gcloud container clusters delete "${CLUSTER_NAME}" \
            --zone "${CLUSTER_ZONE}" \
            --quiet

  postrun:
    name: Post Build
    runs-on: ubuntu-latest
    needs: [teardown-cluster]
    if: failure()
    steps:
      - uses: slackapi/slack-github-action@007b2c3c751a190b6f0f040e47ed024deaa72844 # v1.23.0
        id: slack
        with:
          payload: '{"text": "[wolfictl-push-production] failure: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"}'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK
